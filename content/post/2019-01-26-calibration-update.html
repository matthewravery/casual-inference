---
title: Calibration update, now with Brier Scores!
author: Matthew Avery
date: '2019-01-26'
slug: calibration-update
categories:
  - statistics
tags: []
description: ''
thumbnail: ''
---



<p>After reading my <a href = "/post/is-scott-well-calibrated/">my previous post on calibration</a>, my clever wife (who’s been doing calibration-related activities in the context of modeling and simulation) brought to my attention the concept of <a href = "https://en.wikipedia.org/wiki/Brier_score">Brier Scores</a>. (Alternatively, <a href = "ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf">here</a>.) This approach was originally proposed to evaluate weather forecasts (“Verification of weather forecasts has been a controversial subject for more than half a century,” so at least we’ve moved on controversial climate forecasts in this half-century.)</p>
<p>The math itself is pretty easy for binary predictions (it’s just a squared error loss function, using the probabilities as the “expected value” of an event). Unlike the previous approaches I went through, Brier scores don’t give you a nice hypothesis test. Instead, they give you a relative performance score, allowing you to compare different forecasters on the same set of outcomes. Depending on how loose you want to be with your assumptions, Scott could use this to compare his year-to-year performance and see if he’s getting better, worse, or staying about the same in terms of his prediction skills over time.*</p>
<p>And the R packages <code>scoring</code> has a simple implementation (there really is an R package for just about everything), so we can use that provided we get the data formatted properly. Brier scores are based on individual event observations, so we have to gather the data a bit differently than before. The function I wrote to do this is included at the end of the post for those interested.</p>
<pre class="r"><code>url &lt;- &#39;https://slatestarcodex.com/2019/01/22/2018-predictions-calibration-results/&#39;
library(dplyr)
library(stringr)
library(scoring)
out18 &lt;- get_scotts_results(url) %&gt;% 
  mutate(year = 2018)
out18</code></pre>
<pre><code>## # A tibble: 99 x 4
##    question                                               prob right  year
##    &lt;chr&gt;                                                 &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt;
##  1 1. Donald Trump remains president at end of year: 95%  0.95 TRUE   2018
##  2 2. Democrats take control of the House in midterms: ~  0.8  TRUE   2018
##  3 3. Democrats take control of the Senate in midterms:~  0.5  FALSE  2018
##  4 4. Mueller’s investigation gets cancelled (eg Trump ~  0.5  FALSE  2018
##  5 5. Mueller does not indict Trump: 70%                  0.7  TRUE   2018
##  6 6. PredictIt shows Bernie Sanders having highest cha~  0.6  FALSE  2018
##  7 7. PredictIt shows Donald Trump having highest chanc~  0.95 TRUE   2018
##  8 9. Some sort of major immigration reform legislation~  0.7  FALSE  2018
##  9 10. No major health-care reform legislation gets pas~  0.95 TRUE   2018
## 10 11. No large-scale deportation of Dreamers: 90%        0.9  TRUE   2018
## # ... with 89 more rows</code></pre>
<p>Now that we have the data, lets look at the Brier scores:</p>
<pre class="r"><code>scottbrier18 &lt;- brierscore(right ~ prob, data = out18, decomp = T)$decomp$components[,1]</code></pre>
<pre><code>##                1
## discrim    0.102
## miscal     0.024
## deltaf     0.036
## miscal_lg  0.008
## cov        0.066
## unc        0.457
## brier      0.379
## orig_brier 0.382
## mde_sd     0.409
## mde_median 0.320
## mde_min    0.005
## mde_q25    0.080
## mde_q75    0.500
## mde_max    1.805</code></pre>
<p>So there’s lots of stuff here. <code>discrim</code> is discrimination score, <code>miscal</code> measures the miscalibration of the predictions, and <code>brier</code> is the Brier score. Discrimination gives a forecaster credit for assigning high probabilities to events that end up occurring. Miscalibration (also called “reliability” in the literature) measures how well the forecaster gets the base rate of events correct. This is the closest thing to what I talked about as “calibration” before; basically you’ll get low miscalibration if 80% of your 80% predictions occur. <code>unc</code> is a measure of the total uncertainty in the actual events. This is a measure of the total background uncertainty in the set of events for which there were forecasts. This doesn’t speak directly to the quality of the forecaster but rather to how variable the events were. If you have a high uncertainty value, that means the forecaster has a lot of room to make useful predictions.</p>
<p>By themselves, these don’t mean much, but repeating this process for each year, we can look at trends:</p>
<pre class="r"><code>library(ggplot2)
bind_rows(scottbrier14, scottbrier15, scottbrier16, scottbrier17, scottbrier18) %&gt;% 
  bind_cols(tibble(year = 2014:2018)) %&gt;% 
  tidyr::gather(discrim, miscal, brier, unc, key = &quot;measure&quot;, value = &quot;value&quot;) %&gt;% 
  ggplot(aes(x = year, y = value, colour = measure)) + geom_line() + geom_point(size = 2) + 
  theme_bw() + scale_colour_viridis_d(end = .9)</code></pre>
<p><img src="/post/2019-01-26-calibration-update_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Interpreting this picture is a bit tricky. When looking at Brier scores, smaller values indicate better prediction. The same is true for miscalibration. Discrimination, on the other hand, is better when its higher. Uncertainty gives us an idea of how well the forecaster could do. Because the scales are different, I recommend looking at this next plot instead:</p>
<pre class="r"><code>library(ggplot2)
bind_rows(scottbrier14, scottbrier15, scottbrier16, scottbrier17, scottbrier18) %&gt;% 
  bind_cols(tibble(year = 2014:2018)) %&gt;% 
  tidyr::gather(discrim, miscal, brier, unc, key = &quot;measure&quot;, value = &quot;value&quot;) %&gt;% 
  group_by(measure) %&gt;% 
  mutate(value_scaled = scale(value)) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(x = year, y = value_scaled, colour = measure)) + geom_line() + geom_point(size = 2) + 
  theme_bw() + scale_colour_viridis_d(end = .9)</code></pre>
<p><img src="/post/2019-01-26-calibration-update_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>YMMV, but my reading of this is that Scott’s been pretty consistent, with 2015 being his best year. Despite the apparent dip in performance this year, Scott was actually quite good at assigning high probability to events that occurred despite the relatively high noise this year. His miscalibration also ticked up in 2018 but not enough to matter much. The higher Brier score for this year reflects Scott’s choice of a noisy set of Events more than anything.</p>
<p>*The biggest issue here is data generation. Scott consciously chooses which events to put in his annual posts. He generally chooses things he feels that he’ll be well-calibrated at predicting. If his Brier score changes over time, it’s hard to identify whether that’s because he’s getting better at predicting events, better at knowing which events he can predict well, or if events are just becoming easier to predict in aggregate. There are ways to decompose Brier scores that could help differentiate these questions (<a href = "https://journals.ametsoc.org/doi/pdf/10.1175/1520-0450(1973)012%3C0595%3AANVPOT%3E2.0.CO%3B2">see here</a>; it’s sorta like the bias/variance decomposition for RSME).</p>
<div id="data-scraping-function" class="section level3">
<h3>Data scraping function</h3>
<p>This is the function I used to scrape the results from Scott’s webpage. It’s kinda kludgy in a few places because Scott wasn’t 100% consistent in how he coded things. (e.g., He used colors rather than strike-through in 2014, and he didn’t always include line breaks between predictions.) But as far as I can tell, it works properly.</p>
<pre class="r"><code>get_scotts_results &lt;- function(url){
  
  
  if(sum(c(&quot;dplyr&quot;, &quot;stringr&quot;) %in% (.packages())) &lt; 2){
    stop(&quot;Function requires dplyr and stringr packages to be loaded&quot;)
  }
  
  webpage &lt;- xml2::read_html(url)
  posttext_html &lt;- rvest::html_nodes(webpage,&#39;.pjgm-postcontent&#39;)
  tb &lt;- posttext_html %&gt;% as.character() %&gt;% 
    str_split( &quot;&lt;br&gt;|&lt;/p&gt;\n&lt;p&gt;|6\\. Rep|30\\. 2014|&gt;7\\. Demo|&gt;32\\. HPM&quot;)
  
  tb[[1]] %&gt;% as_tibble() %&gt;% 
    mutate(wrong = str_extract(value, &quot;&lt;s&gt;|&lt;/font&gt;&quot;),
           probbin = str_extract(value, &quot;: \\d{2}&quot;)) %&gt;% 
    filter(!is.na(probbin)) %&gt;% 
    mutate(prob_chr = str_extract(probbin, &quot;\\d{2}&quot;),
           prob = as.numeric(prob_chr)/100,
           right = is.na(wrong),
           question = str_remove_all(value, &quot;\\n|&lt;s&gt;|&lt;/s&gt;&quot;)) %&gt;% 
    select(question, prob, right)
  
}</code></pre>
<p>And as long as we’re here, if you want to read more about Brier scores and the <code>scoring</code> package I used, <a href = "http://journal.sjdm.org/17/17614c/jdm17614c.html">here</a> is a relevant article.</p>
</div>
