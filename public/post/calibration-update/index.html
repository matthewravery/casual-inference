<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Calibration update, now with Brier Scores!</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta name="generator" content="Hugo 0.51" />
	<meta property="og:title" content="Calibration update, now with Brier Scores!" />
<meta property="og:description" content="After reading my my previous post on calibration, my clever wife (who’s been doing calibration-related activities in the context of modeling and simulation) brought to my attention the concept of Brier Scores. (Alternatively, here.) This approach was originally proposed to evaluate weather forecasts (“Verification of weather forecasts has been a controversial subject for more than half a century,” so at least we’ve moved on controversial climate forecasts in this half-century." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/calibration-update/" /><meta property="article:published_time" content="2019-01-26T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-01-26T00:00:00&#43;00:00"/>

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Casual Inference" rel="home">
				<div class="logo__title">Casual Inference</div>
				<div class="logo__tagline">Robust and relaxed</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Home</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/about/">About</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://matthewravery.github.io/markdown-cv">CV</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://github.com/jthaman/ciTools">ciTools</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/analysis-statement/">Analysis Philosophy</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Calibration update, now with Brier Scores!</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-01-26T00:00:00">January 26, 2019</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/statistics" rel="category">statistics</a></span>
</div>
</div>
		</header><div class="content post__content clearfix">
			


<p>After reading my <a href = "/post/is-scott-well-calibrated/">my previous post on calibration</a>, my clever wife (who’s been doing calibration-related activities in the context of modeling and simulation) brought to my attention the concept of <a href = "https://en.wikipedia.org/wiki/Brier_score">Brier Scores</a>. (Alternatively, <a href = "ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf">here</a>.) This approach was originally proposed to evaluate weather forecasts (“Verification of weather forecasts has been a controversial subject for more than half a century,” so at least we’ve moved on controversial climate forecasts in this half-century.)</p>
<div id="brier-scores" class="section level2">
<h2>Brier scores</h2>
<p>The math itself is pretty easy for binary predictions (it’s just a squared error loss function, using the probabilities as the “expected value” of an event). Unlike the previous approaches I went through, Brier scores don’t give you a nice hypothesis test. Instead, they give you a relative performance score, allowing you to compare different forecasters on the same set of outcomes. Depending on how loose you want to be with your assumptions, Scott could use this to compare his year-to-year performance and see if he’s getting better, worse, or staying about the same in terms of his prediction skills over time. There are ways to decompose Brier scores that could help differentiate these questions (<a href = "https://journals.ametsoc.org/doi/pdf/10.1175/1520-0450(1973)012%3C0595%3AANVPOT%3E2.0.CO%3B2">see here</a>; it’s sorta like the bias/variance decomposition for RSME).</p>
<p>And the R packages <code>scoring</code> has a simple implementation (there really is an R package for just about everything), so we can use that provided we get the data formatted properly.</p>
</div>
<div id="scotts-predictions" class="section level2">
<h2>Scott’s predictions</h2>
<p>Brier scores are based on individual event observations, so we have to gather the data a bit differently than before. The function I wrote to do this is included at the end of the post for those interested.</p>
<pre class="r"><code>url &lt;- &#39;https://slatestarcodex.com/2019/01/22/2018-predictions-calibration-results/&#39;
library(dplyr)
library(stringr)
library(scoring)
out18 &lt;- get_scotts_results(url) %&gt;% 
  mutate(year = 2018)
out18</code></pre>
<pre><code>## # A tibble: 99 x 4
##    question                                               prob right  year
##    &lt;chr&gt;                                                 &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt;
##  1 1. Donald Trump remains president at end of year: 95%  0.95 TRUE   2018
##  2 2. Democrats take control of the House in midterms: ~  0.8  TRUE   2018
##  3 3. Democrats take control of the Senate in midterms:~  0.5  FALSE  2018
##  4 4. Mueller’s investigation gets cancelled (eg Trump ~  0.5  FALSE  2018
##  5 5. Mueller does not indict Trump: 70%                  0.7  TRUE   2018
##  6 6. PredictIt shows Bernie Sanders having highest cha~  0.6  FALSE  2018
##  7 7. PredictIt shows Donald Trump having highest chanc~  0.95 TRUE   2018
##  8 9. Some sort of major immigration reform legislation~  0.7  FALSE  2018
##  9 10. No major health-care reform legislation gets pas~  0.95 TRUE   2018
## 10 11. No large-scale deportation of Dreamers: 90%        0.9  TRUE   2018
## # ... with 89 more rows</code></pre>
<p>Now that we have the data, lets look at the Brier scores:</p>
<pre class="r"><code>scottbrier18 &lt;- brierscore(right ~ prob, data = out18, decomp = T)$decomp$components[,1]</code></pre>
<pre><code>##                1
## discrim    0.102
## miscal     0.024
## deltaf     0.036
## miscal_lg  0.008
## cov        0.066
## unc        0.457
## brier      0.379
## orig_brier 0.382
## mde_sd     0.409
## mde_median 0.320
## mde_min    0.005
## mde_q25    0.080
## mde_q75    0.500
## mde_max    1.805</code></pre>
<p>So there’s lots of stuff here. <code>discrim</code> is discrimination score, <code>miscal</code> measures the miscalibration of the predictions, and <code>brier</code> is the Brier score. Discrimination gives a forecaster credit for assigning high probabilities to events that end up occurring. Miscalibration (also called “reliability” in the literature) measures how well the forecaster gets the base rate of events correct. This is the closest thing to what I talked about as “calibration” before; basically you’ll get low miscalibration if 80% of your 80% predictions occur. <code>unc</code> is a measure of the total uncertainty in the actual events. This is a measure of the total background uncertainty in the set of events for which there were forecasts. This doesn’t speak directly to the quality of the forecaster but rather to how variable the events were. If you have a high uncertainty value, that means the forecaster has a lot of room to make useful predictions.</p>
<p>By themselves, these don’t mean much, but repeating this process for each year, we can look at trends:</p>
<pre class="r"><code>library(ggplot2)
bind_rows(scottbrier14, scottbrier15, scottbrier16, scottbrier17, scottbrier18) %&gt;% 
  bind_cols(tibble(year = 2014:2018)) %&gt;% 
  tidyr::gather(discrim, miscal, brier, unc, key = &quot;measure&quot;, value = &quot;value&quot;) %&gt;% 
  ggplot(aes(x = year, y = value, colour = measure)) + geom_line() + geom_point(size = 2) + 
  theme_bw() + scale_colour_viridis_d(end = .9)</code></pre>
<p><img src="/post/2019-01-26-calibration-update_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Interpreting this picture is a bit tricky. When looking at Brier scores, smaller values indicate better prediction. The same is true for miscalibration. Discrimination, on the other hand, is better when its higher. Uncertainty gives us an idea of how well the forecaster could do. Because the scales are different, I recommend looking at this next plot instead:</p>
<pre class="r"><code>library(ggplot2)
bind_rows(scottbrier14, scottbrier15, scottbrier16, scottbrier17, scottbrier18) %&gt;% 
  bind_cols(tibble(year = 2014:2018)) %&gt;% 
  tidyr::gather(discrim, miscal, brier, unc, key = &quot;measure&quot;, value = &quot;value&quot;) %&gt;% 
  group_by(measure) %&gt;% 
  mutate(value_scaled = scale(value)) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(x = year, y = value_scaled, colour = measure)) + geom_line() + geom_point(size = 2) + 
  theme_bw() + scale_colour_viridis_d(end = .9)</code></pre>
<p><img src="/post/2019-01-26-calibration-update_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>YMMV, but my reading of this is that 2018 was an odd year for Scott. He chose a set of events that had substantially more variation in the past, and while he did well at discriminating, he was also more miscalibrated than he’s been in the past. This lead to a much higher Brier score than he usually sees. Prior to that, his aggregate Brier score had been stable despite quite a bit of variation in the components.</p>
</div>
<div id="caveats" class="section level2">
<h2>Caveats</h2>
<p>This brings us to the issue of data generation. While there are plenty of concerns with this exercise (correlation between events being a big one), Scott is also consciously choosing which events to make forecasts on. In the canonical examples for which Brier scores were developed (weather forecasting), the forecaster doesn’t get to pick and choose. So it becomes hard conceptually to interpret the Uncertainty score. Scott chooses the playing field for this game, so he gets to determine if he wants to make predictions on high-uncertainty events or low uncertainty events. Or at least he gets to do this based on his estimated probabilities. This means that his forecast accuracy effects the aggregate uncertainty in the events, insofar as Scott chooses which events to predict based on his forecasts and a desire to have a certain number of events in each bin.</p>
<p>I’ll have to think about this more to decide how it should impact our interpretations of Scott’s scores. I think it would help for Scott to provide details about his process for choosing which events to make forecasts about. I’m reasonably certain its changed the longer he’s done this project, so understanding that could help us understand why 2018 was such an odd-looking year.</p>
</div>
<div id="data-scraping-function" class="section level2">
<h2>Data scraping function</h2>
<p>This is the function I used to scrape the results from Scott’s webpage. It’s kinda kludgy in a few places because Scott wasn’t 100% consistent in how he coded things. (e.g., He used colors rather than strike-through in 2014, and he didn’t always include line breaks between predictions.) But as far as I can tell, it works properly.</p>
<pre class="r"><code>get_scotts_results &lt;- function(url){
  
  
  if(sum(c(&quot;dplyr&quot;, &quot;stringr&quot;) %in% (.packages())) &lt; 2){
    stop(&quot;Function requires dplyr and stringr packages to be loaded&quot;)
  }
  
  webpage &lt;- xml2::read_html(url)
  posttext_html &lt;- rvest::html_nodes(webpage,&#39;.pjgm-postcontent&#39;)
  tb &lt;- posttext_html %&gt;% as.character() %&gt;% 
    str_split( &quot;&lt;br&gt;|&lt;/p&gt;\n&lt;p&gt;|6\\. Rep|30\\. 2014|&gt;7\\. Demo|&gt;32\\. HPM&quot;)
  
  tb[[1]] %&gt;% as_tibble() %&gt;% 
    mutate(wrong = str_extract(value, &quot;&lt;s&gt;|&lt;/font&gt;&quot;),
           probbin = str_extract(value, &quot;: \\d{2}&quot;)) %&gt;% 
    filter(!is.na(probbin)) %&gt;% 
    mutate(prob_chr = str_extract(probbin, &quot;\\d{2}&quot;),
           prob = as.numeric(prob_chr)/100,
           right = is.na(wrong),
           question = str_remove_all(value, &quot;\\n|&lt;s&gt;|&lt;/s&gt;&quot;)) %&gt;% 
    select(question, prob, right)
  
}</code></pre>
<p>And as long as we’re here, if you want to read more about Brier scores and the <code>scoring</code> package I used, <a href = "http://journal.sjdm.org/17/17614c/jdm17614c.html">here</a> is a relevant article.</p>
</div>

		</div>
		
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/is-scott-well-calibrated/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Is Scott well-calibrated?</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/data-generation-processes/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Data Science and Data Generation Processes</p></a>
	</div>
</nav>


			</div>
			


    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/post/diy-metrics-simple-stats-and-game-logs/">DIY Metrics:  Counting up simple statistics</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/diy-metrics-normalizing-by-posession/">DIY Metrics:  Normalizing by Posession</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/diy-metrics-why-normalize/">DIY Metrics:  Why Do we Normalize</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/diy-metrics-full-season-five-man-plus-minus/">DIY Metrics:  Full-season five-man Plus/Minus</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/diy-metrics-preparing-a-new-data-set/">DIY Metrics:  Preparing a new data set</a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/categories/basketball">Basketball</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/common-analysis-errors">Common analysis errors</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/data-generation">Data generation</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/diymetrics">Diymetrics</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/failure-to-communicate">Failure to communicate</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/hot-takes">Hot takes</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/life-is-distributional">Life is distributional</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/metrics">Metrics</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/misc">Misc</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/picking-at-nits">Picking at nits</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/r">R</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/r-diymetrics">R diymetrics</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/sports">Sports</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/statistics">Statistics</a></li>
		</ul>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 Casual Inference.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script></body>
</html>