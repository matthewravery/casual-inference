<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Casual Inference</title>
    <link>/categories/statistics/</link>
    <description>Recent content in Statistics on Casual Inference</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The common failure mode of statistics and economics</title>
      <link>/post/the-common-failure-mode-of-statistics-and-economics/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-common-failure-mode-of-statistics-and-economics/</guid>
      <description>Both Economics and Statistics share a peculiar failure mode: Many critical results in both rely on “large sample”/“long run average” proofs.
The Central Limit Theorem is fundamental to much of classical statitics, including most (if not all) of the fundamental approaches that people are exposed to in their first few courses. The Efficient Market Hypothesis underpins much of the economic theory on which Western economies are based. Both are powerful tools for explaining common phenomena and often make complex problems simpler to understand and model.</description>
    </item>
    
    <item>
      <title>The Crusade Against P-values</title>
      <link>/post/p-values-for-coordination/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/p-values-for-coordination/</guid>
      <description>So we’ll call that break a “summer hiatus”.
But now we’re back, and coming recently from the Joint Statistical Meetings (2019) in Denver, I’ve got Thoughts.
This year’s JSM was different for me, because I spent most of my time on recruitment, speaking with potential applicants during many of the sessions. As a result, I attended many fewer talks that I normally do. By happenstance, the topic of the p-value came up repeatedly in the talks I was able to attend.</description>
    </item>
    
    <item>
      <title>Pie Charts:  A Journey</title>
      <link>/post/pie-charts-a-journey/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pie-charts-a-journey/</guid>
      <description>As a newly-minted PhD Statistician, I was hired by a company that didn’t have a lot of native statistical expertise because they wanted to change that. As a result, I felt empowered to give lots of opinions on topics within my domain to anyone who happened to be in the room, including the head of the division. One of those opinions was that pie charts were the worst.
I viewed pie charts as the scarlet letter of bad analysis: Having one in your analysis should get you shamed and shunned.</description>
    </item>
    
    <item>
      <title>Nonlinearity</title>
      <link>/post/nonlinearity/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/nonlinearity/</guid>
      <description>This is an update to my Analysis Philosphy page, which is still working towards completion
Nonlinearity is a commonly-misunderstood problem when it comes to data analysis, mostly because our profession has once again managed to find a way to use a simple-sounding term in a way that’s counterintuitive to lay audiences. (See also Artificial Intelligence is Dumb.) When people think about nonlinear response variables, they think of functions that have non-linear relationships.</description>
    </item>
    
    <item>
      <title>DIY Metrics: Game Logs </title>
      <link>/post/diy-metrics-game-logs/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-game-logs/</guid>
      <description>Previously on DIY Metircs…Last time in the DIY Metrics series, we had reached the point where we could extract a host of individual metrics from our data set using a function we’d named add_simple_stat_indicators:
add_simple_stat_indicators &amp;lt;- function(tb){tb %&amp;gt;% mutate(gotblk = (description == &amp;quot;BLOCK&amp;quot;),gotstl = (description == &amp;quot;STEAL&amp;quot;),gotast = (description == &amp;quot;ASSIST&amp;quot;),gotreb = map_lgl(description, str_detect, &amp;quot;REBOUND&amp;quot;),tfoulu = map_lgl(description, str_detect, &amp;quot;T.FOUL&amp;quot;),tfoull = map_lgl(description, str_detect, &amp;quot;T.</description>
    </item>
    
    <item>
      <title>How Analytics Ruins Sports</title>
      <link>/post/how-analytics-ruins-sports/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/how-analytics-ruins-sports/</guid>
      <description>With the recent success of the Rockets, people are trotting out that old saw about analytics nerds ruining sports. With the Houston Rockets specifically, the question is a combined referendum on the numbers-based approach of GM Daryl Morey and the foul-drawing proclivities of Houston’s two stars, James Harden and Chris Paul. Of course, the latter is linked with the former, since analytics shows us that drawing shooting fouls is extremely efficient offense.</description>
    </item>
    
    <item>
      <title>ML Invents Metadata</title>
      <link>/post/ml-invent-metadata/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/ml-invent-metadata/</guid>
      <description>Disclaimer: This post is at least tongue-half-way-in-cheek. I acutally like the article I’m lampooning.
A recent publication by academics and AI researchers titled “Data Sheets for Datasets” calls for the Machine Learning community to ensure that all of their datasets are accompanied by a “datasheet.” These datasheets would contain information the dataset’s “motivation, composition,collection process, recommended uses, and so on.” The authors, Gebru, et al., would you like to include more data about your dataset.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Counting up simple statistics</title>
      <link>/post/diy-metrics-simple-stats-and-game-logs/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-simple-stats-and-game-logs/</guid>
      <description>Alrighty! This post got delayed a bit due to real life as well as some challenges with the data. But it’s also an exciting post because we’re finally on the road to generating player-level counting statistics!
Simple StatitisticsThis post is focused on simple counting stats or box score statistics that were basically the standard way to discuss NBA players until quite recently. So aggregate numbers of rebounds, assists, steals, etc.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Normalizing by Posession</title>
      <link>/post/diy-metrics-normalizing-by-posession/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-normalizing-by-posession/</guid>
      <description>As promised, today we’re going to talk about normalizing by possession instead of time on court.
First, a but of motivation. Different teams play at different paces. Some teams try to score a lot in transition, some teams try to slow the ball down and make sure they get good shots in the half-court. Part of this is related to a team’s defense and how quickly they get rebounds in the hands of players who can push the ball.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Why Do we Normalize</title>
      <link>/post/diy-metrics-why-normalize/</link>
      <pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-why-normalize/</guid>
      <description>Until now, we’ve normalized our data by time. This means we’ve been reporting out stats on a “per X minutes” basis. Today, we’re going to unpack a little bit about why we normalize and why we might not always want to normalize by time in the context of the NBA.
What is “normalizing”?Normalization is the act of putting different observations on a level playing field. (That’s not literally what Wikipedia says, but I think it’s a fair paraphrasing for our application.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Full-season five-man Plus/Minus</title>
      <link>/post/diy-metrics-full-season-five-man-plus-minus/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-full-season-five-man-plus-minus/</guid>
      <description>Previously on DIY Metrics, we did some remedial cleaning on the full 17-18 play-by-play data set. Today, we’re going to take that clean data, generate full-season five-man plus/minus numbers, and then do some plotting!
Cleaning, againSo, turns out there were a few bugs that I didn’t catch the first time we went through the cleaning process. This is fairly typical in my experience: You’ll go through your data cleaning and think everything is Gucci only to find once you start your analysis that there are irregularities or issues you hadn’t considered.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Preparing a new data set</title>
      <link>/post/diy-metrics-preparing-a-new-data-set/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-preparing-a-new-data-set/</guid>
      <description>So I finally broke down and got a full season’s worth of NBA play-by-play data to work on. Going forward, I’ll be using the full 2017-2018 play-by-play data from NBAstuffer.
To date, I’ve been building my scripts using functional programming with the goal of having each step easily work with new data sets. This will be a good test of whether I’ve been successful!
But before we do that, we need to look at the new data set and see what, if anything has changed.</description>
    </item>
    
    <item>
      <title>Distributionality</title>
      <link>/post/distributionality/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/distributionality/</guid>
      <description>This is an update to my Analysis Philosphy page, which is still working towards completion
I only get 1,750 hits on Google when I search for “Distributionality”, so maybe I should clarify what I mean, though I don’t think it’s anything profound.
That data follow distributions is a tautology. When this doesn’t appear the case, it means we’ve failed to properly model hte data generation function. The most typical failure mode is to assume that the distribution is simpler than it is.</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Net Ratings (ish)</title>
      <link>/post/diy-metrics-net-ratings-ish/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-net-ratings-ish/</guid>
      <description>Last time on DIY Metrics, we calculated five-man-unit plus/minus ratings from scratch. If we want to use measures like this to compare performance for these groups of players, its important to consider how much game time we have for each unit. There’s a relevant discussion to be had about whether “number of posessions” or “elapsed time” is the best way to compare these groups, (IMO, it depends on what specific question you’re trying to answer with your metric) but today we’ll avoid that discussion and normalize over time because it’s easier.</description>
    </item>
    
    <item>
      <title>&#34;Artificial Intelligence&#34; is dumb</title>
      <link>/post/artificial-intelligence-is-dumb/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/artificial-intelligence-is-dumb/</guid>
      <description>I mean the term of art, not the concept or field of study. And what’s dumb is how it’s applied. “Machine learning” is also dumb in a similar way.
Some definitions for AIYou can go back to the beginning if the field if you want to, but modern definitions tend to to be vague. There are good definitions out there, but these sound esoteric and unless you’re really interested in defining AI precisely, you’ll probably just stick with Merriam-Webster or Wikipedia, which means literally:</description>
    </item>
    
    <item>
      <title>DIY Metrics:  Five-Man Unit Plus/Minus</title>
      <link>/post/diy-metrics-five-man-unit-plus-minus/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics-five-man-unit-plus-minus/</guid>
      <description>Last week, I described how to build a plus/minus score for individual players based on data from NBAstuffer. I enjoyed walking through that process, so lets continue the series and expand our focus.
Five-man units vs. Individual PlayersOne of the first things I talked about on this site was comparing different metrics and choosing the right one for the task at hand. Plus/minus for individual players is a weird metric, because it’s taking a team outcome (net change in score) and applying it at an individual level.</description>
    </item>
    
    <item>
      <title>Local Polynomial Smoothing</title>
      <link>/post/local-polynomial-smoothing/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/local-polynomial-smoothing/</guid>
      <description>So this is a blast from the past. NC State (at least while I was there) did something interesting for their prelim. Instead of taking another test like we had to do at the Masters level, they gave all their students a subject unrelated to their research, and had them write a lit review and do a small simulation study.
My topic was local polynomial smoothing. I don’t think I did a particularly good job, but afterwards, I posted it on my NCSU website as an example of things I’d written.</description>
    </item>
    
    <item>
      <title>DIY Metrics</title>
      <link>/post/diy-metrics/</link>
      <pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diy-metrics/</guid>
      <description>In order to better understand some “advanced metrics”, I figured it’d be useful to build them from scratch. (This is also just a fun exercise in data manipulation, cleaning, etc.)
For starters, let’s do something easy, namely raw plus/minus. For the code below, I’m using the free example play-by-play data set from NBAstuffer. They seem reputable, though I do have concerns about how widely-used their formatting is; one of the challenges with building a workflow is ensuring that the structure of your incoming data won’t change.</description>
    </item>
    
    <item>
      <title>The Arrogance of &#34;Noise&#34;</title>
      <link>/post/the-arrogance-of-noise/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-arrogance-of-noise/</guid>
      <description>This is a post about communication.
One of the through-lines of my academic and professional career is conflict between entrenched subject matter experts (SME) and hot-shot quantitative analysts. As a young undergraduate, I followed Baseball Prospectus Fangraphs through the SABRmetric revolution. I watched Nate Silver bring data-driven prognostication to the world of political journalism which had previously (and arguably still is) dominated by punditry. In my current job, I work with experienced analysts who have often been working on the same systems for years.</description>
    </item>
    
    <item>
      <title>Data Science and Data Generation Processes</title>
      <link>/post/data-generation-processes/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/data-generation-processes/</guid>
      <description>I was talking about a curriculum for a new Data Science degree program, and the topic of experimental design came up. Design of Experiments (DOE) is classical subject area for statisticians, and the context of an applied statistics masters degree makes perfect sense, but in the context of data science, it seemed pretty out of place. I say that not because DOE isn&amp;rsquo;t important but because I think its something &amp;ldquo;data science&amp;rdquo; doesn&amp;rsquo;t often consider.</description>
    </item>
    
    <item>
      <title>Calibration update, now with Brier Scores!</title>
      <link>/post/calibration-update/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/calibration-update/</guid>
      <description>After reading my my previous post on calibration, my clever wife (who’s been doing calibration-related activities in the context of modeling and simulation) brought to my attention the concept of Brier Scores. (Alternatively, here.) This approach was originally proposed to evaluate weather forecasts (“Verification of weather forecasts has been a controversial subject for more than half a century,” so at least we’ve moved on controversial climate forecasts in this half-century.</description>
    </item>
    
    <item>
      <title>Is Scott well-calibrated?</title>
      <link>/post/is-scott-well-calibrated/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/is-scott-well-calibrated/</guid>
      <description>Yesterday, Scott Alexander posted his annual predictions review post. I always enjoy this post because it’s externalized introspection. Scott takes the time to formally look at things he thought, consider how right he was about these things, and consider how it should update his thinking moving forward. Most people don’t do this informally let alone formally!
I want to respond to two things in the post, the latter of which is answering the question Scott only implies of whether he’s well-correlated or not.</description>
    </item>
    
  </channel>
</rss>